---
title: "Assignment 2 - Logistic Regression"
author: "Eoin Flynn"
date: "5 March 2018"
output: pdf_document
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}

---

\centering
Bond University\linebreak
Data Science


\raggedright
\clearpage
\tableofcontents
\clearpage

```{r setup, include=FALSE}
dataScienceReport = T
knitr::opts_chunk$set(echo = dataScienceReport, tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

```{r Functions Text, results='asis', echo=F, include=dataScienceReport}
cat("#Functions
    This section will hold all of the functions that will be used throughout this markdown.")
```
```{r Functions Code}
# Create a decision tree
createDecisionTreeModel <- function(formula, dataset, maxdepth){
  suppressMessages(library(party))
  decisionTreeModel <- ctree(formula, data=dataset, controls = ctree_control(maxdepth = maxdepth))
  
  return(decisionTreeModel)
}

# Change rows to factors
setRowAsFactor <- function(dataset, columns){
  for (column in columns){
    dataset[,column] <- as.factor(dataset[,column])
  }
  return(dataset)
}

# Create a logistic regression model
createLogisticRegressionModel <- function(formula, family=binomial, dataset){
  logisticRegressionModel = glm(formula, family = family, data = dataset)
  
  return(logisticRegressionModel)
}

# Create a prediction dataframe
createPrediction_df <- function(model, dataset, predictionType = "response", oneClass, zeroClass){
  # Run the prediction
  prediction<- predict(model, dataset, type = predictionType)
  # Convert to a dataframe
  prediction_df <- data.frame(prediction)
  # Rename the column to reference easier
  colnames(prediction_df) <- "probabilities"
  # Add a row for the classification
  prediction_df$classification <- rep(zeroClass,nrow(prediction_df))
  # Convert all probabilites above 0.5 to be the affirmative class
  prediction_df$classification[prediction_df$probabilities > 0.5] <- oneClass
  prediction_df$classification <- as.factor(prediction_df$classification)
  
  
  return(prediction_df)
}

# Get model performance for plotting ROC curve
getModelPerformance <- function(model, dataset, outcomeColumn, type="response", xAxis = "tpr", yAxis = "fpr"){
  suppressMessages(library(ROCR))
  # Create a predict variable
  predict <- predict(model, dataset, type = type)
  # Create a predicition variable
  predicition <- prediction(predict, outcomeColumn)
  # Create the performance variable
  performance <- performance(predicition, xAxis, yAxis)
  
  return(performance)
}

# Plot ROC curves
plotROCCurves <- function(model1, model2, main, model1Colour = "#009900", model2Colour = "#FF8000", model1Name, model2Name, legendLocation = "bottomright"){
  plot(model1, main = main, col = model1Colour, print.auc=TRUE)
  plot(model2, add = T, col = model2Colour)
  legend(legendLocation, legend=paste(rep(c(model1Name,model2Name))),col=c(model1Colour, model2Colour),cex=0.8,fill=c(model1Colour, model2Colour))
}

# Calculate AUC. Returns as a decimal
getAUC <- function(outcomeColumn, dataset, model, oneClass, zeroClass){
  suppressMessages(library(ModelMetrics))
  prediction_df <- createPrediction_df(model, dataset, oneClass = oneClass, zeroClass = zeroClass)
  auc <- auc(outcomeColumn, prediction_df$classification)
  
  return(auc)
}

# Create a confusion matrix
createConfusionMatrix <- function(model, dataset, outcomeColumn, oneClass, zeroClass){
  suppressMessages(library(caret))
  prediction_df <- createPrediction_df(model, dataset, oneClass = oneClass, zeroClass = zeroClass)
  userConfusionMatrix <- table(outcomeColumn, prediction_df$classification)
  
  return(userConfusionMatrix)
}
```


```{r Load Data Text, results='asis', echo=F, include=dataScienceReport}
cat("#Data
In this section we will load in our data and do some basic data exploration.")
```
```{r Load Data Code}
suppressMessages(library(RMySQL))

USER <- 'root'
PASSWORD <- 'A13337995'
HOST <- 'localhost'
DBNAME <- 'world'

statement <- "Select * from world.customerChurn"
db <- dbConnect(MySQL(), user = USER, password = PASSWORD, host = HOST, dbname = DBNAME, port=3306)
customerDataset <- dbGetQuery(db, statement = statement)
dbDisconnect(db)

# Loops through and changes all relevant rows to factors and returns the dataset post modification


customerDataset <- setRowAsFactor(customerDataset, c("gender", "SeniorCitizen", "Partner", "Dependents", "PhoneService",
                                                     "MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup",
                                                     "DeviceProtection","TechSupport", "StreamingTV", "StreamingMovies",
                                                     "Contract", "PaperlessBilling", "PaymentMethod", "Churn"
                                                     ))

# Drop the columns that will not be needed
customerDataset = customerDataset[, -which(names(customerDataset) %in% c("customerID"))]

```


```{r Split Text, results='asis', echo=F, include=dataScienceReport}
cat("##Split Data
We will now split our data into test and training sets. The purpose of this is to create a sample of data that the model has never seen before in order to gauge its accuracy. 
The training set will consist of 80% of the data while the remaining 20% will constitute the test set.")
```
```{r Split Code}
suppressMessages(library(caTools))

# Set the seed to reproducability
set.seed(12216)

# Create our two datasets
sample <- sample.split(customerDataset, SplitRatio = .80)
train_df <- subset(customerDataset, sample == TRUE)
test_df <- subset(customerDataset, sample == FALSE)

# We can now see that the data is split approximately 80:20
print(sprintf("The full dataset has %s observations", NROW(customerDataset)))
print(sprintf("The training dataset has %s observations", NROW(train_df)))
print(sprintf("The testing dataset has %s observations", NROW(test_df)))

# Check to see how many customers churned in each dataset
table(train_df$Churn)
table(test_df$Churn)

# We can see that each dataset holds approximately the same proportion of customers who churned
print(sprintf("%.2f%% of the training set churned", ((NROW(subset(train_df, Churn=="Yes")))/NROW(train_df)*100)))
print(sprintf("%.2f%% of the testing set churned", ((NROW(subset(test_df, Churn=="Yes")))/NROW(test_df)*100)))
```


```{r Decision Tree Text, results='asis', echo=F, include=dataScienceReport}
cat("#Decision Tree
We want to first make a decision tree to determine which variables are best able to predict whether a customer will churn. We already know from our previous report
that the optimal model is however this time the tree will only be run on the training dataset.")
```
```{r Decision Tree Code, include=dataScienceReport}
# Create and plot the decision tree
decisionTreeModel = createDecisionTreeModel(formula = Churn~., dataset = train_df, maxdepth = 5)
```
\blandscape
```{r Decision Tree Plot, fig.height=18, fig.width=26, fig.align="centre", include=dataScienceReport}
plot(decisionTreeModel, main = "Decision Tree Model", type = "extended", newpage = TRUE)
```
\elandscape
```{r Decision Tree Discussion, results='asis', echo=F, include=dataScienceReport}
cat("From looking at the decision tree it is clear that the top three variables are Contract, InternetService, and Tenure. Below those three levels, other variables
    such as StreamingTV, and TechSupport become relevant. To develop the best performing model with this dataset we will create a regression using only those three
    top level variables, and then a second using all variables. The results from each model will then be compared before the best model is presented to management.")
```


```{r Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("#Logistic Regression
We will now create multiple logistic regressions using GLM package. We will compare and then optimize before providing a final model to management for business use.")
```

```{r All Variable Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("##All Variables Logistic Regression
In this section we will create a logistic regression using all variables in the training dataset and look at statistical significance of each. Later in the report we will
assess the accuracy of this model against others.")
```
```{r All Variable Logistic Regression, include=dataScienceReport}
# We will start by first making a regression using all variables
allVariablesLogisticRegressionModel <- createLogisticRegressionModel(formula = Churn~., dataset = train_df)

# Print a summary of the regression
print("Model Summary")
summary(allVariablesLogisticRegressionModel)
# Convert Betas into odds ratio
print("Odds Ratios")
exp(coef(allVariablesLogisticRegressionModel))
```
```{r All Variable Logistic Regression Discussion, results='asis', echo=F, include=dataScienceReport}
cat("From the model's summary we can see that our top three variables identified earlier are all have a high statistical significance. TotalCharges is also statistically different from zero however since the decision tree deemed that it had no
    predictive information we will not be including it in our model building")
```

```{r Top Three Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("##Top Three Logistic Regression
We will now create our logistic regression using only the top three variables from our decision tree (Contract, InternetService, and Tenure)")
```
```{r Top Three Regression, include=dataScienceReport}
# We will start by first making a regression using all variables
topThreeLogisticRegressionModel <- createLogisticRegressionModel(formula = Churn~Contract+InternetService+tenure, dataset = train_df)

# Print a summary of the regression
print("Model Summary")
summary(topThreeLogisticRegressionModel)
# Convert Betas into odds ratio
print("Odds Ratios")
exp(coef(topThreeLogisticRegressionModel))
```


```{r Model Comparison Text, results='asis', echo=F, include=dataScienceReport}
cat("#Model Comparison
Now that we have create all three models we will test their accuracy against the training dataset and also the test dataset. One of the easiest ways to see which model
is more accurate is to use an ROC curve and measure the area under each curve, the larger the area under the curve the more accurate the model is. We will first test
each model using the training dataset before testing each one individually with the test dataset to gauge how robust they are. Once we have established whether the models
are robust we will create a pair of confusion matricies using the test dataset to see which model had the highest percentage of churns predicted correctly. ")
```
```{r Model Comparison Code, include=dataScienceReport}
suppressMessages(library(caret))
suppressMessages(library(ROCR))

# Get the performance of each model
allVariableModelPerformance <- getModelPerformance(allVariablesLogisticRegressionModel, dataset = train_df, outcomeColumn = train_df$Churn)
topThreeModelPerformance <- getModelPerformance(topThreeLogisticRegressionModel, dataset = train_df, outcomeColumn = train_df$Churn)

# Plot the ROC curves
plotROCCurves(allVariableModelPerformance, topThreeModelPerformance, main = "ROC Curves Comparison", model1Name = "AllVariableModel", model2Name = "TopThreeModel")

# Get the AUC
allVariableModelAUC <- getAUC(train_df$Churn, train_df, allVariablesLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
topThreeModelAUC <- getAUC(train_df$Churn, train_df, topThreeLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
# Print the AUC
print(sprintf("The AUC for the allVariableModel is %.2f%%", allVariableModelAUC*100))
print(sprintf("The AUC for the topThreeModel is %.2f%%", topThreeModelAUC*100))

# Check the allVariableModel for overfitting
allVariableModelPerformanceTest <- getModelPerformance(allVariablesLogisticRegressionModel, dataset = test_df, outcomeColumn = test_df$Churn)
plotROCCurves(allVariableModelPerformance, allVariableModelPerformanceTest, main = "All Variable Robustness", model1Name = "Training Dataset", model2Name = "Test Dataset")
allVariableModelAUCTest <- getAUC(test_df$Churn, test_df, allVariablesLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
print(sprintf("The AUC for the training dataset using the allVariableModel is %.2f%%", allVariableModelAUC*100))
print(sprintf("The AUC for the test dataset using the allVariableModel is %.2f%%", allVariableModelAUCTest*100))

# Check the topThreeModel for overfitting
topThreeModelPerformanceTest <- getModelPerformance(topThreeLogisticRegressionModel, dataset = test_df, outcomeColumn = test_df$Churn)
plotROCCurves(topThreeModelPerformance, topThreeModelPerformanceTest, main = "Top Three Variable Robustness", model1Name = "Training Dataset", model2Name = "Test Dataset")
topThreeModelAUCTest <- getAUC(test_df$Churn, test_df, topThreeLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
print(sprintf("The AUC for the training dataset using the topThreeModel is %.2f%%", topThreeModelAUC*100))
print(sprintf("The AUC for the test dataset using the topThreeModel is %.2f%%", topThreeModelAUCTest*100))

# Create confusion matricies to compare predicitons
print("All Variable Model Confusion Matrix")
createConfusionMatrix(allVariablesLogisticRegressionModel, test_df, test_df$Churn, oneClass = "Yes", zeroClass = "No")
print("Top Three Variable Confusion Matrix")
print(createConfusionMatrix(topThreeLogisticRegressionModel, test_df, test_df$Churn, oneClass = "Yes", zeroClass = "No"))
```






