---
title: "Assignment 2 - Logistic Regression"
author: "Eoin Flynn"
date: "5 March 2018"
output: pdf_document
header-includes:
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}

---

\centering
Bond University\linebreak
Data Science


\raggedright
\clearpage
\tableofcontents
\clearpage

```{r setup, include=FALSE}
dataScienceReport = T
knitr::opts_chunk$set(echo = dataScienceReport, tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# Introduction
In this report we will develop a logistic regression model that will build upon our decision tree from our previous report. Once created, the model will allow us to predict whether customers outside of our dataset will Churn and with what liklihood. 

```{r Functions Text, results='asis', echo=F, include=dataScienceReport}
cat("#Functions
    This section will hold all of the functions that will be used throughout this markdown.")
```
```{r Functions Code}
# Create a decision tree
createDecisionTreeModel <- function(formula, dataset, maxdepth){
  suppressMessages(library(party))
  decisionTreeModel <- ctree(formula, data=dataset, controls = ctree_control(maxdepth = maxdepth))
  
  return(decisionTreeModel)
}

# Change rows to factors
setRowAsFactor <- function(dataset, columns){
  for (column in columns){
    dataset[,column] <- as.factor(dataset[,column])
  }
  return(dataset)
}

# Create a logistic regression model
createLogisticRegressionModel <- function(formula, family=binomial, dataset){
  logisticRegressionModel = glm(formula, family = family, data = dataset)
  
  return(logisticRegressionModel)
}

# Create a prediction dataframe
createPrediction_df <- function(model, dataset, predictionType = "response", oneClass, zeroClass){
  # Run the prediction
  prediction<- suppressWarnings(predict(model, dataset, type = predictionType))
  # Convert to a dataframe
  prediction_df <- data.frame(prediction)
  # Rename the column to reference easier
  colnames(prediction_df) <- "probabilities"
  # Add a row for the classification
  prediction_df$classification <- rep(zeroClass,nrow(prediction_df))
  # Convert all probabilites above 0.5 to be the affirmative class
  prediction_df$classification[prediction_df$probabilities > 0.5] <- oneClass
  prediction_df$classification <- as.factor(prediction_df$classification)
  
  
  return(prediction_df)
}

# Get model performance for plotting ROC curve
getModelPerformance <- function(model, dataset, outcomeColumn, type="response", xAxis = "tpr", yAxis = "fpr"){
  suppressMessages(library(ROCR))
  # Create a predict variable
  predict <- predict(model, dataset, type = type)
  # Create a predicition variable
  predicition <- prediction(predict, outcomeColumn)
  # Create the performance variable
  performance <- performance(predicition, xAxis, yAxis)
  
  return(performance)
}

# Plot ROC curves
plotROCCurves <- function(model1, model2, main, model1Colour = "#009900", model2Colour = "#FF8000", model1Name, model2Name, legendLocation = "bottomright"){
  plot(model1, main = main, col = model1Colour, print.auc=TRUE)
  plot(model2, add = T, col = model2Colour)
  legend(legendLocation, legend=paste(rep(c(model1Name,model2Name))),col=c(model1Colour, model2Colour),cex=0.8,fill=c(model1Colour, model2Colour))
}

# Calculate AUC. Returns as a decimal
getAUC <- function(outcomeColumn, dataset, model, oneClass, zeroClass){
  suppressMessages(library(ModelMetrics))
  prediction_df <- createPrediction_df(model, dataset, oneClass = oneClass, zeroClass = zeroClass)
  auc <- auc(outcomeColumn, prediction_df$classification)
  
  return(auc)
}

# Create a confusion matrix. Returns a confusion matrix
createConfusionMatrix <- function(model, dataset, outcomeColumn, oneClass, zeroClass){
  suppressMessages(library(caret))
  prediction_df <- createPrediction_df(model, dataset, oneClass = oneClass, zeroClass = zeroClass)
  userConfusionMatrix <- table(outcomeColumn, prediction_df$classification)
  
  return(userConfusionMatrix)
}

# Create a new customer for predicition. Returns a dataframe
createCustomer <- function(originalDataset, gender, SeniorCitizen, Partner, Dependents, tenure, PhoneService, MultipleLines, InternetService, OnlineSecurity,
                           OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, MontlyCharges,
                           TotalCharges, Churn){
  # Create a copy of the original dataset and keep one row that will be overridden with the new data.
  newCustomer <- customerDataset[1,]
  
  newCustomer$gender <- gender
  newCustomer$SeniorCitizen <- SeniorCitizen
  newCustomer$Partner<- Partner
  newCustomer$Dependents <- Dependents
  newCustomer$tenure <- tenure
  newCustomer$PhoneService <- PhoneService
  newCustomer$MultipleLines <- MultipleLines
  newCustomer$InternetService <- InternetService
  newCustomer$OnlineSecurity <- OnlineSecurity
  newCustomer$OnlineBackup <- OnlineBackup
  newCustomer$DeviceProtection <- DeviceProtection
  newCustomer$TechSupport <- TechSupport
  newCustomer$StreamingTV <- StreamingTV
  newCustomer$StreamingMovies <- StreamingMovies
  newCustomer$Contract <- Contract
  newCustomer$PaperlessBilling <- PaperlessBilling
  newCustomer$PaymentMethod <- PaymentMethod
  newCustomer$MonthlyCharges <- MontlyCharges
  newCustomer$TotalCharges <- TotalCharges
  newCustomer$Churn <- Churn
  
  # Convert fields that are factors
  newCustomer <- setRowAsFactor(newCustomer, c("gender", "SeniorCitizen", "Partner", "Dependents", "PhoneService",
                                                     "MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup",
                                                     "DeviceProtection","TechSupport", "StreamingTV", "StreamingMovies",
                                                     "Contract", "PaperlessBilling", "PaymentMethod", "Churn"
                                                     ))
  
  return(newCustomer)
                           }
```


```{r Load Data Text, results='asis', echo=F, include=dataScienceReport}
cat("#Data
In this section we will load in our data and do some basic data exploration.")
```
```{r Load Data Code, include=dataScienceReport}
suppressMessages(library(RMySQL))

USER <- 'root'
PASSWORD <- 'A13337995'
HOST <- 'localhost'
DBNAME <- 'world'

statement <- "Select * from world.customerChurn"
db <- dbConnect(MySQL(), user = USER, password = PASSWORD, host = HOST, dbname = DBNAME, port=3306)
customerDataset <- dbGetQuery(db, statement = statement)
dbDisconnect(db)

# Loops through and changes all relevant rows to factors and returns the dataset post modification


customerDataset <- setRowAsFactor(customerDataset, c("gender", "SeniorCitizen", "Partner", "Dependents", "PhoneService",
                                                     "MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup",
                                                     "DeviceProtection","TechSupport", "StreamingTV", "StreamingMovies",
                                                     "Contract", "PaperlessBilling", "PaymentMethod", "Churn"
                                                     ))

# Drop the columns that will not be needed
customerDataset <- customerDataset[, -which(names(customerDataset) %in% c("customerID"))]

```


```{r Split Text, results='asis', echo=F, include=dataScienceReport}
cat("##Split Data
We will now split our data into test and training sets. The purpose of this is to create a sample of data that the model has never seen before in order to gauge its accuracy. 
The training set will consist of 80% of the data while the remaining 20% will constitute the test set.")
```
```{r Split Code, include=dataScienceReport}
suppressMessages(library(caTools))

# Set the seed to reproducability
set.seed(12216)

# Create our two datasets
sample <- sample.split(customerDataset, SplitRatio = .80)
train_df <- subset(customerDataset, sample == TRUE)
test_df <- subset(customerDataset, sample == FALSE)

# We can now see that the data is split approximately 80:20
print(sprintf("The full dataset has %s observations", NROW(customerDataset)))
print(sprintf("The training dataset has %s observations", NROW(train_df)))
print(sprintf("The testing dataset has %s observations", NROW(test_df)))

# Check to see how many customers churned in each dataset
table(train_df$Churn)
table(test_df$Churn)

# We can see that each dataset holds approximately the same proportion of customers who churned
print(sprintf("%.2f%% of the training set churned", ((NROW(subset(train_df, Churn=="Yes")))/NROW(train_df)*100)))
print(sprintf("%.2f%% of the testing set churned", ((NROW(subset(test_df, Churn=="Yes")))/NROW(test_df)*100)))
```


```{r Decision Tree Text, results='asis', echo=F, include=dataScienceReport}
cat("#Decision Tree
We want to first make a decision tree to determine which variables are best able to predict whether a customer will churn. We already know from our previous report
that the optimal model is however this time the tree will only be run on the training dataset.")
```
```{r Decision Tree Code, include=dataScienceReport}
# Create and plot the decision tree
decisionTreeModel = createDecisionTreeModel(formula = Churn~., dataset = train_df, maxdepth = 5)
```
\blandscape
```{r Decision Tree Plot, fig.height=18, fig.width=26, fig.align="centre", include=dataScienceReport}
plot(decisionTreeModel, main = "Decision Tree Model", type = "extended", newpage = TRUE)
```
\elandscape
```{r Decision Tree Discussion, results='asis', echo=F, include=dataScienceReport}
cat("###Decision Tree Results
From looking at the decision tree it is clear that the top three variables are Contract, InternetService, and Tenure. Below those three levels, other variables
    such as StreamingTV, and TechSupport become relevant. To develop the best performing model with this dataset we will create a regression using only those three
    top level variables, and then a second using all variables. The results from each model will then be compared before the best model is presented to management.")
```


```{r Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("#Logistic Regression
We will now create multiple logistic regressions using GLM package. We will compare and then optimize before providing a final model to management for business use.")
```

```{r All Variable Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("##All Variables Logistic Regression
In this section we will create a logistic regression using all variables in the training dataset and look at statistical significance of each. Later in the report we will
assess the accuracy of this model against others.")
```
```{r All Variable Logistic Regression, include=dataScienceReport}
# We will start by first making a regression using all variables
allVariablesLogisticRegressionModel <- createLogisticRegressionModel(formula = Churn~., dataset = train_df)

# Print a summary of the regression
print("Model Summary")
summary(allVariablesLogisticRegressionModel)
```
```{r All Variable Logistic Regression Discussion, results='asis', echo=F, include=dataScienceReport}
cat("###All Variables Logistic Regression Results
From the model's summary we can see that our top three variables identified earlier are all have a high statistical significance. TotalCharges is also statistically different from zero however since the decision tree deemed that it had no
    predictive information we will not be including it in our model building")
```

```{r Top Three Logistic Regression Text, results='asis', echo=F, include=dataScienceReport}
cat("##Top Three Logistic Regression
We will now create our logistic regression using only the top three variables from our decision tree (Contract, InternetService, and Tenure)")
```
```{r Top Three Regression, include=dataScienceReport}
# We will start by first making a regression using all variables
topThreeLogisticRegressionModel <- createLogisticRegressionModel(formula = Churn~Contract+InternetService+tenure, dataset = train_df)

# Print a summary of the regression
print("Model Summary")
summary(topThreeLogisticRegressionModel)
```


```{r Model Comparison Text, results='asis', echo=F, include=dataScienceReport}
cat("#Model Comparison
Now that we have created our two models we will test their accuracy against the training dataset and also the test dataset. One of the easiest ways to see which model
is more accurate is to use an ROC curve and measure the area under each curve, the larger the area under the curve the more accurate the model is. We will first test
each model using the training dataset before testing each one individually with the test dataset to gauge how robust they are. Once we have established whether the models
are robust we will create a pair of confusion matricies using the test dataset to see which model had the highest percentage of churns predicted correctly. ")
```
```{r Model Comparison Code, include=dataScienceReport}
suppressMessages(library(caret))
suppressMessages(library(ROCR))

# Get the performance of each model
allVariableModelPerformance <- getModelPerformance(allVariablesLogisticRegressionModel, dataset = train_df, outcomeColumn = train_df$Churn)
topThreeModelPerformance <- getModelPerformance(topThreeLogisticRegressionModel, dataset = train_df, outcomeColumn = train_df$Churn)

# Get the AUC
allVariableModelAUC <- getAUC(train_df$Churn, train_df, allVariablesLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
topThreeModelAUC <- getAUC(train_df$Churn, train_df, topThreeLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")

# Plot the ROC curves
plotROCCurves(allVariableModelPerformance, topThreeModelPerformance, main = "ROC Curves Comparison", 
              model1Name = sprintf("All Variable Model. AUC %.2f%%", allVariableModelAUC*100), 
              model2Name = sprintf("Top Three Model. AUC %.2f%%", topThreeModelAUC*100))

# Check the allVariableModel for overfitting
allVariableModelPerformanceTest <- getModelPerformance(allVariablesLogisticRegressionModel, dataset = test_df, outcomeColumn = test_df$Churn)
allVariableModelAUCTest <- getAUC(test_df$Churn, test_df, allVariablesLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
plotROCCurves(allVariableModelPerformance, allVariableModelPerformanceTest, main = "All Variable Robustness", 
              model1Name = sprintf("Training Dataset. AUC %.2f%%", allVariableModelAUC*100), 
              model2Name = sprintf("Test Dataset. AUC %.2f%%", allVariableModelAUCTest*100))

# Check the topThreeModel for overfitting
topThreeModelPerformanceTest <- getModelPerformance(topThreeLogisticRegressionModel, dataset = test_df, outcomeColumn = test_df$Churn)
topThreeModelAUCTest <- getAUC(test_df$Churn, test_df, topThreeLogisticRegressionModel, oneClass = "Yes", zeroClass = "No")
plotROCCurves(topThreeModelPerformance, topThreeModelPerformanceTest, main = "Top Three Variable Robustness", 
              model1Name = sprintf("Training Dataset. AUC %.2f%%", topThreeModelAUC*100), 
              model2Name = sprintf("Test Dataset. AUC %.2f%%", topThreeModelAUCTest*100))


# Create confusion matricies to compare predicitons
print("All Variable Model Confusion Matrix")
allVariableConfusionMatrix <- createConfusionMatrix(allVariablesLogisticRegressionModel, test_df, test_df$Churn, oneClass = "Yes", zeroClass = "No")
allVariableConfusionMatrix
allVariableYesCorrectPercentage <- allVariableConfusionMatrix[2,2]/sum(allVariableConfusionMatrix)
sprintf("The All Variable Model predicted yes correctly %.2f%% of the time", allVariableYesCorrectPercentage*100)
print("Top Three Variable Confusion Matrix")
topThreeConfusionMatrix <- createConfusionMatrix(topThreeLogisticRegressionModel, test_df, test_df$Churn, oneClass = "Yes", zeroClass = "No")
topThreeConfusionMatrix
topThreeYesCorrectPercentage <- topThreeConfusionMatrix[2,2]/sum(topThreeConfusionMatrix)
sprintf("The Top Three Model predicted yes correctly %.2f%% of the time", topThreeYesCorrectPercentage*100)

```
```{r Model Comparison Discussion, results='asis', echo=F, include=dataScienceReport}
cat("###Model Comparison Discussion
Looking at the ROC Curves Comparison graph we can see that the two models have a similar AUC with the All Variable Model only slightly outperforming the Top Three Model.
The following two ROC curves show us that both models are robust and have not been overfit since they are approximately as accurate of the test dataset as they are on
the training dataset.
In line with the results from the ROC curve, the All Variable model also out performs the topThreeModel in terms of the highest percentage of Churns predicited correctly.
For these reasons we will be presenting the All Variable Model to management.")
```


#Model
After testing different combinations of predictive factors we were able to produce this model which accurately predicts the whether a customer will churn 14.32% of the time. For example, if we take a customer who is on a month-to-month plan, has fiber optic internet, and has been with the company for one year, we can feed that information into the model a predict how likely they are to Churn.
```{r New Customer, echo=dataScienceReport}
# Create a new customer
newCustomer <- createCustomer(customerDataset, 
                              gender = "Female", 
                              SeniorCitizen = 0, 
                              Partner = "Yes", 
                              Dependents = "No", 
                              tenure = 1, 
                              PhoneService = "Yes",
                              MultipleLines = "No", 
                              InternetService = "Fiber optic",
                              OnlineSecurity = "No",
                              OnlineBackup = "No",
                              DeviceProtection = "No",
                              TechSupport = "No",
                              StreamingTV = "Yes",
                              StreamingMovies = "Yes",
                              Contract = "Month-to-month",
                              PaperlessBilling = "Yes",
                              PaymentMethod = "Bank transfer (automatic)",
                              MontlyCharges = 34.5,
                              TotalCharges = 49,
                              Churn = "Yes"
                              )
newCustomerPrediciton <- createPrediction_df(allVariablesLogisticRegressionModel, dataset = newCustomer, oneClass = "Yes", zeroClass = "No")
newCustomerPrediciton
```
The model has predicited that a customer with these parameters has a 96.17% liklihood of churning. To understand this result a better we should turn our attention to the odds ratios produced by the model.
```{r Odds Ratio, echo=dataScienceReport}
print("Odds Ratios")
exp(coef(allVariablesLogisticRegressionModel))
```
We can see from these statistics that being on the Fiber optic network makes a customer 5.844 times more likely to churn as opposed to not being on it. A main driver behind this statistic could be sub-optimal network speeds. The type of customer who uses a fiber optic network is someone who needs the fastest possible internet speeds, whether it be for business or personal use. It could be worth the company's time and resources to further invest into this network to reduce the liklihood of a customer churning. Not offering the network is illadvised since customers will opt for other providers who do offer the network, even if its speeds are also below expectations.



